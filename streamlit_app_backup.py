import streamlit as st
from io import BytesIO
from collections import Counter
from pypdf import PdfReader
import re
import unicodedata
from openai import OpenAI
import json
import os
from pathlib import Path
import plotly.express as px
import plotly.graph_objects as go

# ìƒìˆ˜ ì •ì˜
MAX_FILE_SIZE_MB = 20  # Streamlit ê¸°ë³¸ê°’ë³´ë‹¤ ì•ˆì „í•˜ê²Œ ì„¤ì •
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024

# API í‚¤ ê´€ë¦¬
CONFIG_DIR = Path(__file__).parent / "config"
CONFIG_FILE = CONFIG_DIR / "api_keys.json"

def load_api_key():
    """API í‚¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    # 1. í™˜ê²½ ë³€ìˆ˜ì—ì„œ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        return api_key
    
    # 2. Streamlit secretsì—ì„œ í™•ì¸
    try:
        if hasattr(st, 'secrets') and 'OPENAI_API_KEY' in st.secrets:
            return st.secrets['OPENAI_API_KEY']
    except:
        pass
    
    # 3. ì„¤ì • íŒŒì¼ì—ì„œ í™•ì¸
    try:
        if CONFIG_FILE.exists():
            with open(CONFIG_FILE, 'r') as f:
                config = json.load(f)
                return config.get('openai_api_key')
    except:
        pass
    
    return None

def save_api_key(api_key):
    """API í‚¤ë¥¼ ì„¤ì • íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        CONFIG_DIR.mkdir(exist_ok=True)
        config = {'openai_api_key': api_key}
        with open(CONFIG_FILE, 'w') as f:
            json.dump(config, f, indent=2)
        return True
    except Exception as e:
        st.error(f"API í‚¤ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return False

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@st.cache_resource
def get_openai_client():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    api_key = load_api_key()
    if not api_key:
        return None
    return OpenAI(api_key=api_key)

# ==================== GPT ê¸°ë°˜ ë¶„ì„ í•¨ìˆ˜ ====================
def gpt_summarize(text, max_words=3000):
    """GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ì„ ìš”ì•½í•©ë‹ˆë‹¤."""
    try:
        client = get_openai_client()
        if not client:
            return {"error": "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì œí•œ ê³ ë ¤)
        words = text.split()
        truncated_text = ' '.join(words[:max_words])
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ìˆ  ë…¼ë¬¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆì  ì—°êµ¬ë°©ë²•ë¡ ì— íŠ¹íˆ ì •í†µí•©ë‹ˆë‹¤."},
                {"role": "user", "content": f"""ë‹¤ìŒ í•™ìˆ  ë…¼ë¬¸ì„ ë¶„ì„í•˜ì—¬ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

{truncated_text}

ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "í•µì‹¬ìš”ì•½": "3-5ë¬¸ì¥ì˜ í•µì‹¬ ë‚´ìš© ìš”ì•½",
  "ì—°êµ¬ëª©ì ": "ì—°êµ¬ì˜ ëª©ì ê³¼ ë°°ê²½",
  "ì—°êµ¬ë°©ë²•": "ì‚¬ìš©ëœ ì—°êµ¬ë°©ë²•ë¡  (ì§ˆì /ì–‘ì /í˜¼í•© ë“±)",
  "ì£¼ìš”ë°œê²¬": "í•µì‹¬ ì—°êµ¬ ê²°ê³¼",
  "ì´ë¡ ì ê¸°ì—¬": "ì´ë¡ ì /ì‹¤ì²œì  í•¨ì˜",
  "í•œê³„ì ": "ì—°êµ¬ì˜ í•œê³„ì "
}}"""}
            ],
            temperature=0.3,
            max_tokens=1500
        )
        
        result = response.choices[0].message.content
        # JSON íŒŒì‹± ì‹œë„
        try:
            return json.loads(result)
        except:
            # JSONì´ ì•„ë‹ˆë©´ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return {"í•µì‹¬ìš”ì•½": result}
    except Exception as e:
        return {"error": f"GPT ë¶„ì„ ì‹¤íŒ¨: {str(e)}"}

def gpt_extract_themes(text, max_words=2000):
    """GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        client = get_openai_client()
        if not client:
            return {"error": "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        words = text.split()
        truncated_text = ' '.join(words[:max_words])
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì§ˆì ì—°êµ¬ ì½”ë”© ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ì£¼ì œ(theme)ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

{truncated_text}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "ì£¼ìš”ì£¼ì œ": ["ì£¼ì œ1", "ì£¼ì œ2", "ì£¼ì œ3", "ì£¼ì œ4", "ì£¼ì œ5"],
  "í•µì‹¬ê°œë…": ["ê°œë…1", "ê°œë…2", "ê°œë…3", "ê°œë…4", "ê°œë…5"]
}}"""}
            ],
            temperature=0.3,
            max_tokens=500
        )
        
        result = response.choices[0].message.content
        try:
            return json.loads(result)
        except:
            return {"ì£¼ìš”ì£¼ì œ": [], "í•µì‹¬ê°œë…": []}
    except Exception as e:
        return {"error": f"ì£¼ì œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"}

def gpt_compare_papers(paper_texts, max_words_per_paper=1500):
    """GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ë…¼ë¬¸ì„ ë¹„êµí•©ë‹ˆë‹¤."""
    try:
        client = get_openai_client()
        if not client:
            return {"error": "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        # ê° ë…¼ë¬¸ì—ì„œ ì¼ë¶€ë§Œ ì¶”ì¶œ
        truncated_papers = {}
        for name, text in paper_texts.items():
            words = text.split()
            truncated_papers[name] = ' '.join(words[:max_words_per_paper])
        
        papers_text = "\n\n".join([f"[ë…¼ë¬¸: {name}]\n{text}" for name, text in truncated_papers.items()])
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ìˆ  ë…¼ë¬¸ ë¹„êµë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": f"""ë‹¤ìŒ ë…¼ë¬¸ë“¤ì„ ë¹„êµ ë¶„ì„í•˜ì„¸ìš”:

{papers_text}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "ê³µí†µì£¼ì œ": ["ì£¼ì œ1", "ì£¼ì œ2", "ì£¼ì œ3"],
  "ì°¨ë³„ì ": "ê° ë…¼ë¬¸ì˜ ì£¼ìš” ì°¨ë³„ì ",
  "ë°©ë²•ë¡ ë¹„êµ": "ì—°êµ¬ë°©ë²•ë¡ ì˜ ìœ ì‚¬ì ê³¼ ì°¨ì´ì ",
  "ì¢…í•©í‰ê°€": "ì „ì²´ì ì¸ ë¹„êµ í‰ê°€"
}}"""}
            ],
            temperature=0.3,
            max_tokens=1000
        )
        
        result = response.choices[0].message.content
        try:
            return json.loads(result)
        except:
            return {"ì¢…í•©í‰ê°€": result}
    except Exception as e:
        return {"error": f"ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"}

def gpt_research_questions(text, max_words=2000):
    """GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°êµ¬ì§ˆë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        client = get_openai_client()
        if not client:
            return {"error": "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        words = text.split()
        truncated_text = ' '.join(words[:max_words])
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ìˆ  ë…¼ë¬¸ì˜ ì—°êµ¬ì§ˆë¬¸ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": f"""ë‹¤ìŒ ë…¼ë¬¸ì—ì„œ ì—°êµ¬ì§ˆë¬¸(Research Questions)ì„ ì¶”ì¶œí•˜ì„¸ìš”:

{truncated_text}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "ì—°êµ¬ì§ˆë¬¸": ["RQ1", "RQ2", "RQ3"],
  "ì—°êµ¬ê°€ì„¤": ["H1", "H2"] 
}}

ì—°êµ¬ì§ˆë¬¸ì´ ëª…ì‹œì ìœ¼ë¡œ ì—†ë‹¤ë©´ ë…¼ë¬¸ì˜ ëª©ì ì„ ë°”íƒ•ìœ¼ë¡œ ì¶”ë¡ í•˜ì„¸ìš”."""}
            ],
            temperature=0.3,
            max_tokens=500
        )
        
        result = response.choices[0].message.content
        try:
            return json.loads(result)
        except:
            return {"ì—°êµ¬ì§ˆë¬¸": [], "ì—°êµ¬ê°€ì„¤": []}
    except Exception as e:
        return {"error": f"ì—°êµ¬ì§ˆë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"}
# ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (í™•ì¥)
STOP_WORDS = set([
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
    'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
    'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
    'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those',
    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'what', 'which', 'who',
    'when', 'where', 'why', 'how', 'all', 'each', 'every', 'both', 'few',
    'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',
    'own', 'same', 'so', 'than', 'too', 'very', 'also', 'into', 'through',
    'during', 'before', 'after', 'above', 'below', 'up', 'down', 'out', 'off',
    'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there',
    'however', 'therefore', 'thus', 'furthermore', 'moreover', 'nevertheless',
    'although', 'though', 'whereas', 'while', 'since', 'because', 'unless',
    'whether', 'either', 'neither', 'rather', 'between', 'among', 'within'
])

# ==================== ê³ ê¸‰ í…ìŠ¤íŠ¸ ë¶„ì„ í•¨ìˆ˜ ====================

def analyze_readability(text):
    """í…ìŠ¤íŠ¸ ê°€ë…ì„±ì„ ë‹¤ì–‘í•œ ì§€í‘œë¡œ ë¶„ì„í•©ë‹ˆë‹¤."""
    try:
        # Flesch Reading Ease (0-100, ë†’ì„ìˆ˜ë¡ ì½ê¸° ì‰¬ì›€)
        flesch_reading = textstat.flesch_reading_ease(text)
        
        # Flesch-Kincaid Grade Level (í•™ë…„ ìˆ˜ì¤€)
        fk_grade = textstat.flesch_kincaid_grade(text)
        
        # SMOG Index (ì´í•´ì— í•„ìš”í•œ êµìœ¡ ì—°ìˆ˜)
        smog = textstat.smog_index(text)
        
        # Coleman-Liau Index
        coleman_liau = textstat.coleman_liau_index(text)
        
        # Automated Readability Index
        ari = textstat.automated_readability_index(text)
        
        # Dale-Chall Readability Score
        dale_chall = textstat.dale_chall_readability_score(text)
        
        # í‰ê· ê°’ ê³„ì‚°
        avg_grade = (fk_grade + smog + coleman_liau + ari) / 4
        
        # í•´ì„
        if flesch_reading >= 60:
            difficulty = "ì‰¬ì›€"
        elif flesch_reading >= 30:
            difficulty = "ë³´í†µ"
        else:
            difficulty = "ì–´ë ¤ì›€"
        
        return {
            'flesch_reading_ease': round(flesch_reading, 2),
            'flesch_kincaid_grade': round(fk_grade, 2),
            'smog_index': round(smog, 2),
            'coleman_liau': round(coleman_liau, 2),
            'ari': round(ari, 2),
            'dale_chall': round(dale_chall, 2),
            'average_grade_level': round(avg_grade, 2),
            'difficulty': difficulty
        }
    except Exception as e:
        return None

def analyze_sentence_complexity(text):
    """ë¬¸ì¥ ë³µì¡ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    try:
        sentences = sent_tokenize(text)
        if not sentences:
            return None
        
        # ë¬¸ì¥ ê¸¸ì´ ë¶„ì„
        sentence_lengths = [len(word_tokenize(s)) for s in sentences]
        
        # ë‹¨ì–´ ê¸¸ì´ ë¶„ì„
        words = word_tokenize(text.lower())
        word_lengths = [len(w) for w in words if w.isalpha()]
        
        # ì–´íœ˜ ë‹¤ì–‘ì„± (Type-Token Ratio)
        unique_words = len(set(words))
        total_words = len(words)
        ttr = (unique_words / total_words * 100) if total_words > 0 else 0
        
        # ê¸´ ë‹¨ì–´ ë¹„ìœ¨ (7ì ì´ìƒ)
        long_words = [w for w in words if len(w) >= 7 and w.isalpha()]
        long_word_ratio = (len(long_words) / len(words) * 100) if len(words) > 0 else 0
        
        return {
            'avg_sentence_length': round(np.mean(sentence_lengths), 2),
            'max_sentence_length': max(sentence_lengths),
            'min_sentence_length': min(sentence_lengths),
            'sentence_length_std': round(np.std(sentence_lengths), 2),
            'avg_word_length': round(np.mean(word_lengths), 2),
            'vocabulary_diversity': round(ttr, 2),
            'long_word_ratio': round(long_word_ratio, 2),
            'total_sentences': len(sentences),
            'total_words': len(words),
            'unique_words': unique_words
        }
    except Exception as e:
        return None

def extract_collocations(text, n=20):
    """í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ë‹¨ì–´ ì¡°í•©(collocation)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        # í…ìŠ¤íŠ¸ í† í°í™”
        words = word_tokenize(text.lower())
        words = [w for w in words if w.isalpha() and len(w) > 3 and w not in STOP_WORDS]
        
        if len(words) < 20:
            return []
        
        # Bigram Collocation Finder
        bigram_measures = BigramAssocMeasures()
        finder = BigramCollocationFinder.from_words(words)
        
        # ìµœì†Œ ë¹ˆë„ í•„í„° (3ë²ˆ ì´ìƒ ì¶œí˜„)
        finder.apply_freq_filter(3)
        
        # PMI (Pointwise Mutual Information) ê¸°ë°˜ ìƒìœ„ collocation
        collocations = finder.nbest(bigram_measures.pmi, n)
        
        # ë¹ˆë„ìˆ˜ì™€ í•¨ê»˜ ë°˜í™˜
        collocation_freq = []
        for col in collocations:
            freq = finder.ngram_fd[col]
            collocation_freq.append((' '.join(col), freq))
        
        return collocation_freq
    except Exception as e:
        return []

def build_cooccurrence_network(text, top_n=30):
    """ë‹¨ì–´ ê³µë™ ì¶œí˜„ ë„¤íŠ¸ì›Œí¬ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
    try:
        sentences = sent_tokenize(text)
        
        # ë‹¨ì–´ ê³µë™ ì¶œí˜„ í–‰ë ¬ ìƒì„±
        cooccurrence = defaultdict(lambda: defaultdict(int))
        
        for sentence in sentences[:200]:  # ì²˜ìŒ 200ë¬¸ì¥ë§Œ ì‚¬ìš© (ë©”ëª¨ë¦¬ íš¨ìœ¨)
            words = word_tokenize(sentence.lower())
            words = [w for w in words if w.isalpha() and len(w) > 3 and w not in STOP_WORDS]
            
            # ê°™ì€ ë¬¸ì¥ì— ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ìŒ
            for i, word1 in enumerate(words):
                for word2 in words[i+1:]:
                    cooccurrence[word1][word2] += 1
                    cooccurrence[word2][word1] += 1
        
        # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
        G = nx.Graph()
        
        # ìƒìœ„ ë¹ˆë„ ë‹¨ì–´ ì„ íƒ
        all_words = Counter()
        for word, cowords in cooccurrence.items():
            all_words[word] += sum(cowords.values())
        
        top_words = [word for word, _ in all_words.most_common(top_n)]
        
        # ì—£ì§€ ì¶”ê°€
        for word1 in top_words:
            for word2 in top_words:
                if word1 != word2 and word2 in cooccurrence[word1]:
                    weight = cooccurrence[word1][word2]
                    if weight >= 2:  # ìµœì†Œ 2ë²ˆ ì´ìƒ ê³µë™ ì¶œí˜„
                        G.add_edge(word1, word2, weight=weight)
        
        return G
    except Exception as e:
        return None

def extract_topics_lda(text, n_topics=5, n_words=10):
    """LDA í† í”½ ëª¨ë¸ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        # ë¬¸ì¥ ë¶„í• 
        sentences = sent_tokenize(text)
        
        if len(sentences) < 10:
            return None
        
        # TF-IDF ë²¡í„°í™”
        vectorizer = TfidfVectorizer(
            max_features=200,
            stop_words='english',
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.8
        )
        
        tfidf_matrix = vectorizer.fit_transform(sentences)
        
        # LDA ëª¨ë¸ í•™ìŠµ
        lda = LatentDirichletAllocation(
            n_components=n_topics,
            random_state=42,
            max_iter=50,
            learning_method='batch'
        )
        
        lda.fit(tfidf_matrix)
        
        # í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ
        feature_names = vectorizer.get_feature_names_out()
        topics = []
        
        for topic_idx, topic in enumerate(lda.components_):
            top_indices = topic.argsort()[-n_words:][::-1]
            top_words = [feature_names[i] for i in top_indices]
            top_scores = [topic[i] for i in top_indices]
            
            topics.append({
                'topic_id': topic_idx + 1,
                'words': top_words,
                'scores': [round(float(s), 4) for s in top_scores]
            })
        
        return topics
    except Exception as e:
        return None

def calculate_semantic_similarity(text1, text2):
    """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        vectorizer = TfidfVectorizer(
            stop_words='english',
            ngram_range=(1, 2),
            min_df=1
        )
        
        # TF-IDF ë²¡í„°í™”
        tfidf_matrix = vectorizer.fit_transform([text1, text2])
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        
        return round(similarity * 100, 2)
    except Exception as e:
        return None

def analyze_discourse_markers(text):
    """ë‹´í™” í‘œì§€(discourse markers)ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    discourse_categories = {
        'ì¸ê³¼ê´€ê³„': ['because', 'therefore', 'thus', 'hence', 'consequently', 'as a result', 'due to', 'since'],
        'ëŒ€ì¡°': ['however', 'but', 'although', 'despite', 'nevertheless', 'on the other hand', 'whereas', 'while', 'yet'],
        'ì¶”ê°€': ['furthermore', 'moreover', 'additionally', 'also', 'in addition', 'besides', 'likewise'],
        'ì˜ˆì‹œ': ['for example', 'for instance', 'such as', 'including', 'namely', 'specifically'],
        'ê²°ë¡ ': ['in conclusion', 'to conclude', 'in summary', 'to sum up', 'overall', 'finally'],
        'ê°•ì¡°': ['indeed', 'in fact', 'actually', 'certainly', 'clearly', 'obviously']
    }
    
    text_lower = text.lower()
    results = {}
    
    for category, markers in discourse_categories.items():
        count = sum(text_lower.count(marker) for marker in markers)
        results[category] = count
    
    return results

def extract_citation_patterns(text):
    """ì¸ìš© íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
    patterns = {
        'author_year': r'\([A-Z][a-z]+(?:\s+et al\.)?,?\s+\d{4}\)',
        'author_year_page': r'\([A-Z][a-z]+(?:\s+et al\.)?,?\s+\d{4},?\s+p+\.\s*\d+\)',
        'numbered': r'\[\d+\]',
        'multiple_authors': r'et al\.',
    }
    
    results = {}
    for pattern_name, pattern in patterns.items():
        matches = re.findall(pattern, text)
        results[pattern_name] = len(matches)
    
    return results

# ==================== í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ====================
def clean_text(text):
    """í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•˜ê³  ì •ê·œí™”í•©ë‹ˆë‹¤."""
    # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
    text = unicodedata.normalize('NFKD', text)
    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    # í•˜ì´í”ˆìœ¼ë¡œ ë‚˜ë‰œ ë‹¨ì–´ ë³µì›
    text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text)
    return text.strip()

# ==================== PDF ë¡œë“œ ====================
def load_pdf_from_upload(uploaded_file):
    """ì—…ë¡œë“œëœ PDF íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = uploaded_file.size
        file_size_mb = file_size / 1024 / 1024
        
        if file_size == 0:
            return None, "âŒ ì—…ë¡œë“œëœ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ PDF íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
        
        if file_size > MAX_FILE_SIZE_BYTES:
            return None, f"âŒ íŒŒì¼ í¬ê¸°ê°€ {MAX_FILE_SIZE_MB}MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.\ní˜„ì¬ íŒŒì¼: {file_size_mb:.2f}MB\n\nğŸ’¡ í•´ê²° ë°©ë²•:\n- PDF ì••ì¶• ë„êµ¬ ì‚¬ìš© (ì˜ˆ: smallpdf.com)\n- ë¶ˆí•„ìš”í•œ ì´ë¯¸ì§€ ì œê±°\n- í•„ìš”í•œ í˜ì´ì§€ë§Œ ì¶”ì¶œ"
        
        # ê²½ê³  ë©”ì‹œì§€ (15MB ì´ìƒ)
        if file_size_mb > 15:
            import streamlit as st
            st.warning(f"âš ï¸ íŒŒì¼ í¬ê¸°ê°€ í½ë‹ˆë‹¤ ({file_size_mb:.2f}MB). ì²˜ë¦¬ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # BytesIOë¡œ ë³€í™˜ - íŒŒì¼ í¬ì¸í„°ë¥¼ ì²˜ìŒìœ¼ë¡œ ë¦¬ì…‹
        uploaded_file.seek(0)
        content = BytesIO(uploaded_file.read())
        content.seek(0)
        
        # íŒŒì¼ì´ ì‹¤ì œë¡œ PDFì¸ì§€ í™•ì¸
        header = content.read(4)
        content.seek(0)
        if header != b'%PDF':
            return None, "âŒ ìœ íš¨í•œ PDF íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤. PDF íŒŒì¼ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        return content, None
    except Exception as e:
        return None, f"âŒ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}"

# ==================== í…ìŠ¤íŠ¸ ì¶”ì¶œ ====================
def extract_text(pdf_file):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    try:
        # PDF íŒŒì¼ í¬ì¸í„°ë¥¼ ì²˜ìŒìœ¼ë¡œ ë¦¬ì…‹
        pdf_file.seek(0)
        
        reader = PdfReader(pdf_file)
        
        # PDFê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if len(reader.pages) == 0:
            return None, None, "âŒ PDF íŒŒì¼ì— í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = {
            'pages': len(reader.pages),
            'title': None,
            'author': None,
            'subject': None,
            'creator': None
        }
        
        if reader.metadata:
            metadata['title'] = reader.metadata.get('/Title', None)
            metadata['author'] = reader.metadata.get('/Author', None)
            metadata['subject'] = reader.metadata.get('/Subject', None)
            metadata['creator'] = reader.metadata.get('/Creator', None)
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = ""
        for i, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n\n"
            except Exception as e:
                # ê°œë³„ í˜ì´ì§€ ì¶”ì¶œ ì‹¤íŒ¨ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                continue
        
        if not text or len(text.strip()) < 100:
            return None, None, "âŒ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ê¸°ë°˜ PDFì´ê±°ë‚˜ ë³´í˜¸ëœ íŒŒì¼ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        text = clean_text(text)
        
        return text, metadata, None
    except Exception as e:
        error_msg = str(e)
        if "empty file" in error_msg.lower():
            return None, None, "âŒ ë¹ˆ íŒŒì¼ì´ê±°ë‚˜ ì†ìƒëœ PDFì…ë‹ˆë‹¤. ë‹¤ë¥¸ íŒŒì¼ì„ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "encrypted" in error_msg.lower():
            return None, None, "âŒ ì•”í˜¸í™”ëœ PDFì…ë‹ˆë‹¤. ì•”í˜¸ë¥¼ í•´ì œí•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        else:
            return None, None, f"âŒ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {error_msg}"

# ==================== ìš”ì•½ ìƒì„± ====================
def extract_sentences(text):
    """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤."""
    # ë¬¸ì¥ ì¢…ê²° íŒ¨í„´ ê°œì„ 
    sentences = re.split(r'(?<=[.!?])\s+(?=[A-Zê°€-í£])', text)
    # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ë§Œ í•„í„°ë§ (ìµœì†Œ 30ì)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 30]
    return sentences

def summarize(text):
    """í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    sentences = extract_sentences(text)
    
    if len(sentences) == 0:
        empty_summary = {
            'executive': "ì¶”ì¶œí•  ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.",
            'structured': "ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.",
            'sections': {},
            'word_count': 0,
            'sentence_count': 0
        }
        return empty_summary
    
    # ê¸°ë³¸ í†µê³„
    word_count = len(text.split())
    sentence_count = len(sentences)
    
    # í•µì‹¬ ìš”ì•½ (ì²˜ìŒ 5-7 ë¬¸ì¥)
    executive_length = min(7, max(5, len(sentences) // 10))
    executive = " ".join(sentences[:executive_length])
    
    # êµ¬ì¡°í™”ëœ ìš”ì•½ (ì²˜ìŒ 12-15 ë¬¸ì¥)
    structured_length = min(15, max(12, len(sentences) // 5))
    structured = " ".join(sentences[:structured_length])
    
    # ì„¹ì…˜ë³„ ë¶„ì„ (ì§ˆì ì—°êµ¬ë°©ë²•ë¡  ê´€ë ¨)
    sections = identify_sections(text, sentences)
    
    return {
        'executive': executive,
        'structured': structured,
        'sections': sections,
        'word_count': word_count,
        'sentence_count': sentence_count
    }

def identify_sections(text, sentences):
    """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ì„¹ì…˜ì„ ì‹ë³„í•©ë‹ˆë‹¤."""
    sections = {
        'ì—°êµ¬ ëª©ì  ë° ë°°ê²½': {'keywords': ['purpose', 'objective', 'aim', 'goal', 'background', 'introduction', 'context', 'ëª©ì ', 'ë°°ê²½', 'ì„œë¡ '], 'content': []},
        'ì´ë¡ ì  í”„ë ˆì„ì›Œí¬': {'keywords': ['theory', 'theoretical', 'framework', 'perspective', 'lens', 'paradigm', 'ì´ë¡ ', 'í”„ë ˆì„ì›Œí¬', 'ê´€ì '], 'content': []},
        'ì—°êµ¬ ë°©ë²•': {'keywords': ['method', 'methodology', 'approach', 'design', 'procedure', 'data collection', 'participant', 'sample', 'ë°©ë²•', 'ì—°êµ¬ì„¤ê³„', 'ì°¸ì—¬ì', 'ìë£Œìˆ˜ì§‘'], 'content': []},
        'ìë£Œ ë¶„ì„': {'keywords': ['analysis', 'coding', 'theme', 'category', 'pattern', 'interpretation', 'ë¶„ì„', 'ì½”ë”©', 'ì£¼ì œ', 'ë²”ì£¼'], 'content': []},
        'ì—°êµ¬ ê²°ê³¼': {'keywords': ['result', 'finding', 'outcome', 'emerged', 'revealed', 'discovered', 'ê²°ê³¼', 'ë°œê²¬'], 'content': []},
        'ë…¼ì˜ ë° í•¨ì˜': {'keywords': ['discussion', 'implication', 'significance', 'contribution', 'limitation', 'future', 'ë…¼ì˜', 'í•¨ì˜', 'ì˜ì˜', 'í•œê³„'], 'content': []}
    }
    
    # ì„¹ì…˜ í—¤ë” íƒì§€
    text_lower = text.lower()
    section_positions = []
    
    for section_name, section_data in sections.items():
        for keyword in section_data['keywords']:
            # ì„¹ì…˜ í—¤ë”ë¡œ ë³´ì´ëŠ” íŒ¨í„´ ì°¾ê¸°
            pattern = rf'\n\s*{re.escape(keyword)}[s]?\s*\n'
            matches = re.finditer(pattern, text_lower, re.IGNORECASE)
            for match in matches:
                section_positions.append((match.start(), section_name))
    
    # ìœ„ì¹˜ìˆœ ì •ë ¬
    section_positions.sort()
    
    # ê° ì„¹ì…˜ì˜ ë‚´ìš© ì¶”ì¶œ
    for i, (pos, section_name) in enumerate(section_positions):
        start_pos = pos
        end_pos = section_positions[i + 1][0] if i + 1 < len(section_positions) else len(text)
        
        section_text = text[start_pos:end_pos]
        section_sentences = extract_sentences(section_text)
        
        # ì²˜ìŒ 3-5 ë¬¸ì¥ ì €ì¥
        sections[section_name]['content'] = section_sentences[:5]
    
    # í—¤ë”ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­
    for section_name, section_data in sections.items():
        if not section_data['content']:
            for sent in sentences[:100]:  # ì²˜ìŒ 100ë¬¸ì¥ë§Œ ê²€ì‚¬
                sent_lower = sent.lower()
                keyword_count = sum(1 for kw in section_data['keywords'] if kw in sent_lower)
                if keyword_count >= 2:  # 2ê°œ ì´ìƒì˜ í‚¤ì›Œë“œ ë§¤ì¹­
                    idx = sentences.index(sent)
                    section_data['content'] = sentences[idx:min(idx+3, len(sentences))]
                    break
    
    return sections

# ==================== í‚¤ì›Œë“œ ì¶”ì¶œ ====================
def analyze_keywords(text, top_n=20):
    """TF-IDFì™€ ë¹ˆë„ ë¶„ì„ì„ ê²°í•©í•˜ì—¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        # í…ìŠ¤íŠ¸ ì •ì œ
        words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())
        
        if len(words) < 20:
            return {'tfidf': [], 'frequency': [], 'academic': []}
        
        # TF-IDF í‚¤ì›Œë“œ
        tfidf_keywords = []
        try:
            vectorizer = TfidfVectorizer(
                max_features=top_n,
                stop_words='english',
                ngram_range=(1, 2),
                min_df=2
            )
            tfidf_matrix = vectorizer.fit_transform([text])
            feature_names = vectorizer.get_feature_names_out()
            scores = tfidf_matrix.toarray()[0]
            
            tfidf_keywords = sorted(
                zip(feature_names, scores),
                key=lambda x: x[1],
                reverse=True
            )[:top_n]
        except:
            pass
        
        # ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ (ë¶ˆìš©ì–´ ì œì™¸)
        word_freq = Counter([w for w in words if w not in STOP_WORDS and len(w) > 4])
        frequency_keywords = word_freq.most_common(top_n)
        
        # í•™ìˆ  ìš©ì–´ íƒì§€ (ì§ˆì ì—°êµ¬ë°©ë²•ë¡  ê´€ë ¨)
        academic_terms = [
            'qualitative', 'quantitative', 'methodology', 'phenomenology',
            'grounded theory', 'case study', 'ethnography', 'narrative',
            'interview', 'observation', 'participant', 'coding', 'theme',
            'category', 'analysis', 'interpretation', 'trustworthiness',
            'credibility', 'transferability', 'dependability', 'confirmability',
            'triangulation', 'saturation', 'reflexivity', 'rigor', 'validity',
            'reliability', 'framework', 'theoretical', 'empirical', 'context'
        ]
        
        found_terms = []
        text_lower = text.lower()
        for term in academic_terms:
            count = text_lower.count(term)
            if count > 0:
                found_terms.append((term, count))
        
        found_terms.sort(key=lambda x: x[1], reverse=True)
        
        return {
            'tfidf': tfidf_keywords,
            'frequency': frequency_keywords,
            'academic': found_terms[:15]
        }
    except Exception as e:
        return {'tfidf': [], 'frequency': [], 'academic': []}

# ==================== ì°¸ê³ ë¬¸í—Œ ë¶„ì„ ====================
def analyze_references(text):
    """ì°¸ê³ ë¬¸í—Œì„ ì¶”ì¶œí•˜ê³  ìƒì„¸ ë¶„ì„í•©ë‹ˆë‹¤."""
    # References ì„¹ì…˜ ì°¾ê¸°
    ref_patterns = [
        r'References\s*\n(.*?)(?=\n\n[A-Z][a-z]+|\Z)',
        r'Bibliography\s*\n(.*?)(?=\n\n[A-Z][a-z]+|\Z)',
        r'References\s*\n(.*)',
        r'REFERENCES\s*\n(.*)',
        r'ì°¸ê³ ë¬¸í—Œ\s*\n(.*)'
    ]
    
    ref_section = ""
    for pattern in ref_patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            ref_section = match.group(1)[:10000]  # ì²˜ìŒ 10000ì
            break
    
    if not ref_section:
        return {
            'items': [],
            'count': 0,
            'years': {},
            'avg_authors': 0,
            'recent_ratio': 0,
            'oldest_year': None,
            'newest_year': None,
            'journal_types': {}
        }
    
    # ì°¸ê³ ë¬¸í—Œ í•­ëª© ì¶”ì¶œ
    ref_lines = []
    for line in ref_section.split('\n'):
        line = line.strip()
        # ì˜ë¯¸ìˆëŠ” ì°¸ê³ ë¬¸í—Œ ë¼ì¸ (ìµœì†Œ 50ì, ìˆ«ìë‚˜ íŠ¹ìˆ˜ë¬¸ì í¬í•¨)
        if len(line) > 50 and re.search(r'[0-9]', line):
            ref_lines.append(line)
    
    # ì—°ë„ ë¶„ì„
    years = []
    for line in ref_lines:
        year_matches = re.findall(r'\b(19[5-9]\d|20[0-2]\d)\b', line)
        if year_matches:
            years.extend([int(y) for y in year_matches])
    
    year_dist = Counter(years)
    
    # ìµœê·¼ ë…¼ë¬¸ ë¹„ìœ¨ (ìµœê·¼ 5ë…„)
    from datetime import datetime
    current_year = datetime.now().year
    recent_years = [y for y in years if y >= current_year - 5]
    recent_ratio = (len(recent_years) / len(years) * 100) if years else 0
    
    # ì €ì ìˆ˜ ë¶„ì„
    total_authors = 0
    author_counts = []
    
    for line in ref_lines[:50]:  # ì²˜ìŒ 50ê°œë§Œ ìƒì„¸ ë¶„ì„
        # ì €ì íŒ¨í„´ íƒì§€
        authors = 0
        
        # íŒ¨í„´ 1: "Last, F., Last, F., & Last, F."
        comma_pattern = len(re.findall(r',\s*[A-Z]\.', line))
        authors += comma_pattern
        
        # íŒ¨í„´ 2: "and" ë˜ëŠ” "&"
        and_pattern = len(re.findall(r'\s+(?:and|&)\s+[A-Z]', line, re.IGNORECASE))
        authors += and_pattern
        
        # íŒ¨í„´ 3: "et al."
        if 'et al' in line.lower():
            authors += 3  # et al. ìˆìœ¼ë©´ ìµœì†Œ 3ëª… ì´ìƒ
        
        if authors > 0:
            author_counts.append(authors)
            total_authors += authors
    
    avg_authors = (total_authors / len(author_counts)) if author_counts else 0
    
    # ì €ë„/ì¶œíŒë¬¼ ìœ í˜• ë¶„ì„
    journal_indicators = {
        'ì €ë„ ë…¼ë¬¸': ['journal', 'vol.', 'volume', 'pp.', 'pages', 'issue'],
        'í•™ìˆ ëŒ€íšŒ': ['conference', 'proceedings', 'symposium', 'workshop'],
        'ë‹¨í–‰ë³¸': ['book', 'press', 'publisher', 'edition'],
        'í•™ìœ„ë…¼ë¬¸': ['dissertation', 'thesis', 'phd', 'doctoral', 'master']
    }
    
    journal_types = defaultdict(int)
    for line in ref_lines:
        line_lower = line.lower()
        for j_type, indicators in journal_indicators.items():
            if any(indicator in line_lower for indicator in indicators):
                journal_types[j_type] += 1
                break
    
    return {
        'items': ref_lines[:20],  # ìƒìœ„ 20ê°œë§Œ ì €ì¥
        'count': len(ref_lines),
        'years': dict(year_dist.most_common(15)),
        'avg_authors': round(avg_authors, 1),
        'recent_ratio': round(recent_ratio, 1),
        'oldest_year': min(years) if years else None,
        'newest_year': max(years) if years else None,
        'journal_types': dict(journal_types)
    }

# ==================== ë…¼ë¬¸ ë¹„êµ ====================
def compare_papers(papers_data):
    """ì—¬ëŸ¬ ë…¼ë¬¸ì„ ì²´ê³„ì ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤."""
    if len(papers_data) < 2:
        return None
    
    comparison = {}
    
    # ê¸°ë³¸ í†µê³„ ë¹„êµ
    comparison['basic_stats'] = []
    for name, data in papers_data.items():
        stats = {
            'ë…¼ë¬¸': name,
            'í˜ì´ì§€': data.get('metadata', {}).get('pages', 'N/A'),
            'ë‹¨ì–´ ìˆ˜': f"{data['summary']['word_count']:,}",
            'ë¬¸ì¥ ìˆ˜': data['summary']['sentence_count'],
            'ì°¸ê³ ë¬¸í—Œ': data['references']['count']
        }
        comparison['basic_stats'].append(stats)
    
    # í‚¤ì›Œë“œ ë¹„êµ
    all_tfidf = []
    all_academic = []
    
    for name, data in papers_data.items():
        keywords = data.get('keywords', {})
        if keywords.get('tfidf'):
            all_tfidf.extend([kw[0] for kw in keywords['tfidf'][:10]])
        if keywords.get('academic'):
            all_academic.extend([kw[0] for kw in keywords['academic'][:10]])
    
    common_tfidf = [kw for kw, count in Counter(all_tfidf).items() if count > 1]
    common_academic = [kw for kw, count in Counter(all_academic).items() if count > 1]
    
    comparison['common_keywords'] = {
        'tfidf': common_tfidf[:15],
        'academic': common_academic[:15]
    }
    
    # ì°¸ê³ ë¬¸í—Œ ë¹„êµ
    ref_comparison = []
    for name, data in papers_data.items():
        refs = data['references']
        ref_stats = {
            'ë…¼ë¬¸': name,
            'ì°¸ê³ ë¬¸í—Œ ìˆ˜': refs['count'],
            'í‰ê·  ì €ì ìˆ˜': refs['avg_authors'],
            'ìµœê·¼ 5ë…„ ë¹„ìœ¨': f"{refs['recent_ratio']}%",
            'ì—°ë„ ë²”ìœ„': f"{refs['oldest_year']}-{refs['newest_year']}" if refs['oldest_year'] else 'N/A'
        }
        ref_comparison.append(ref_stats)
    
    comparison['references'] = ref_comparison
    
    # ì—°êµ¬ë°©ë²•ë¡  ìš©ì–´ ë¹„êµ
    method_terms = ['qualitative', 'quantitative', 'mixed method', 'case study',
                    'grounded theory', 'phenomenology', 'ethnography', 'interview',
                    'survey', 'observation', 'coding', 'theme']
    
    method_presence = {}
    for name, data in papers_data.items():
        text_lower = data['text'].lower()
        found = [term for term in method_terms if term in text_lower]
        method_presence[name] = found
    
    comparison['methodology'] = method_presence
    
    return comparison

# ==================== Streamlit UI ====================
def main():
    st.set_page_config(
        page_title="í•™ìˆ  ë…¼ë¬¸ ë¶„ì„ ë„êµ¬",
        page_icon="ğŸ“š",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # ì»¤ìŠ¤í…€ CSS
    st.markdown("""
        <style>
        .main-header {
            font-size: 2.5rem;
            font-weight: 700;
            color: #1f77b4;
            margin-bottom: 0.5rem;
        }
        .sub-header {
            font-size: 1.1rem;
            color: #666;
            margin-bottom: 2rem;
        }
        .metric-card {
            background-color: #f0f2f6;
            padding: 1rem;
            border-radius: 0.5rem;
            margin: 0.5rem 0;
        }
        .section-header {
            color: #1f77b4;
            border-bottom: 2px solid #1f77b4;
            padding-bottom: 0.5rem;
            margin-top: 1.5rem;
            margin-bottom: 1rem;
        }
        </style>
    """, unsafe_allow_html=True)
    
    st.markdown('<div class="main-header">ğŸ“š AI í•™ìˆ  ë…¼ë¬¸ ë¶„ì„ ë„êµ¬</div>', unsafe_allow_html=True)
    st.markdown('<div class="sub-header">GPT-4 ê¸°ë°˜ ì§ˆì ì—°êµ¬ë°©ë²•ë¡  ëŒ€í•™ì›ìƒì„ ìœ„í•œ ì§€ëŠ¥í˜• PDF ë¶„ì„ ì‹œìŠ¤í…œ</div>', unsafe_allow_html=True)
    
    # API í‚¤ ìƒíƒœ í‘œì‹œ
    api_key = load_api_key()
    if api_key:
        st.success("ğŸ”‘ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'papers' not in st.session_state:
        st.session_state.papers = {}
    
    # ì‚¬ì´ë“œë°”: PDF ì—…ë¡œë“œ
    with st.sidebar:
        st.header("ğŸ“¤ PDF ì—…ë¡œë“œ")
        
        with st.expander("â„¹ï¸ ì‚¬ìš© ê°€ì´ë“œ", expanded=False):
            st.markdown("""
            **ğŸ“Š ê¸°ë³¸ ë¶„ì„ ê¸°ëŠ¥ (í•­ìƒ ì‹¤í–‰):**
            - êµ¬ì¡°í™”ëœ ìš”ì•½ ìƒì„±
            - TF-IDF í‚¤ì›Œë“œ ë¶„ì„
            - ì°¸ê³ ë¬¸í—Œ ì‹¬ì¸µ ë¶„ì„
            
            **ğŸ¤– AI ê³ ê¸‰ ë¶„ì„ (ì„ íƒì  ì‹¤í–‰):**
            - GPT-4 ì§€ëŠ¥í˜• ìš”ì•½
            - ì§ˆì ì—°êµ¬ ì£¼ì œ(Theme) ì¶”ì¶œ
            - ì—°êµ¬ì§ˆë¬¸ ë° ê°€ì„¤ ì‹ë³„
            - ë‹¤ì¤‘ ë…¼ë¬¸ ë¹„êµ ë¶„ì„
            
            **ğŸ“ íŒŒì¼ í¬ê¸° ê¶Œì¥ì‚¬í•­:**
            - ê¶Œì¥: 10MB ì´í•˜ (ë¹ ë¥¸ ì²˜ë¦¬)
            - ìµœëŒ€: 20MB
            - 413 ì—ëŸ¬ ë°œìƒ ì‹œ: PDF ì••ì¶• í•„ìš”
            
            **ğŸ’¡ íŒŒì¼ í¬ê¸° ì¤„ì´ê¸°:**
            1. smallpdf.comì—ì„œ PDF ì••ì¶•
            2. ë¶ˆí•„ìš”í•œ í˜ì´ì§€ ì œê±°
            3. ì´ë¯¸ì§€ í’ˆì§ˆ ë‚®ì¶”ê¸°
            """)
        
        st.markdown(f"**ğŸ“Š íŒŒì¼ í¬ê¸° ì œí•œ: {MAX_FILE_SIZE_MB}MB**")
        st.caption("âš ï¸ íŒŒì¼ì´ ë„ˆë¬´ í¬ë©´ ì—…ë¡œë“œê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. PDFë¥¼ ì••ì¶•í•˜ê±°ë‚˜ í˜ì´ì§€ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”.")
        
        uploaded_file = st.file_uploader(
            "PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
            type=['pdf'],
            help=f"í•™ìˆ  ë…¼ë¬¸ PDF íŒŒì¼ (ê¶Œì¥: 10MB ì´í•˜, ìµœëŒ€: {MAX_FILE_SIZE_MB}MB)"
        )
        
        paper_name = st.text_input(
            "ë…¼ë¬¸ ì œëª© (ì„ íƒì‚¬í•­)",
            placeholder="ì˜ˆ: Smith et al. (2023) - Qualitative Study",
            help="ë¹„ì›Œë‘ë©´ íŒŒì¼ëª…ì´ ì‚¬ìš©ë©ë‹ˆë‹¤"
        )
        
        analyze_button = st.button("ğŸ” ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True)
        
        if analyze_button:
            if not uploaded_file:
                st.error("âŒ PDF íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            else:
                # íŒŒì¼ í¬ê¸° ë¯¸ë¦¬ ì²´í¬
                file_size_mb = uploaded_file.size / 1024 / 1024
                
                if file_size_mb > MAX_FILE_SIZE_MB:
                    st.error(f"""
                    âŒ íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤!
                    
                    **í˜„ì¬ íŒŒì¼:** {file_size_mb:.2f}MB  
                    **ìµœëŒ€ í—ˆìš©:** {MAX_FILE_SIZE_MB}MB
                    
                    **ğŸ’¡ í•´ê²° ë°©ë²•:**
                    1. [smallpdf.com](https://smallpdf.com/kr/compress-pdf)ì—ì„œ PDF ì••ì¶•
                    2. ë¶ˆí•„ìš”í•œ í˜ì´ì§€ ì œê±°
                    3. Adobe Acrobatì—ì„œ "íŒŒì¼ í¬ê¸° ì¤„ì´ê¸°" ì‚¬ìš©
                    """)
                else:
                    with st.spinner("ğŸ“„ PDF ì²˜ë¦¬ ì¤‘..."):
                        pdf_content, error = load_pdf_from_upload(uploaded_file)
            if not uploaded_file:
                st.error("âŒ PDF íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            else:
                with st.spinner("ğŸ“„ PDF ì²˜ë¦¬ ì¤‘..."):
                    pdf_content, error = load_pdf_from_upload(uploaded_file)
                    
                    if error:
                        st.error(error)
                    else:
                        with st.spinner("ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
                            text, metadata, extract_error = extract_text(pdf_content)
                            
                            if extract_error:
                                st.error(extract_error)
                            else:
                                if len(text) < 500:
                                    st.error("âŒ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. PDFê°€ ì†ìƒë˜ì—ˆê±°ë‚˜ ì´ë¯¸ì§€ ê¸°ë°˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                                else:
                                    # ë¶„ì„ ìˆ˜í–‰
                                    progress_bar = st.progress(0)
                                    status_text = st.empty()
                                    
                                    status_text.text("ğŸ“Š ê¸°ë³¸ ë¶„ì„ ì¤‘...")
                                    progress_bar.progress(20)
                                    summary = summarize(text)
                                    
                                    status_text.text("ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
                                    progress_bar.progress(50)
                                    keywords = analyze_keywords(text)
                                    
                                    status_text.text("ğŸ“š ì°¸ê³ ë¬¸í—Œ ë¶„ì„ ì¤‘...")
                                    progress_bar.progress(60)
                                    references = analyze_references(text)
                                    
                                    status_text.text("ğŸ“Š ê³ ê¸‰ í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘...")
                                    progress_bar.progress(75)
                                    readability = analyze_readability(text)
                                    complexity = analyze_sentence_complexity(text)
                                    collocations = extract_collocations(text)
                                    discourse = analyze_discourse_markers(text)
                                    citations = extract_citation_patterns(text)
                                    topics_lda = extract_topics_lda(text)
                                    
                                    progress_bar.progress(90)
                                    
                                    # ì €ì¥ (GPT ë¶„ì„ì€ ë‚˜ì¤‘ì— ì„ íƒì ìœ¼ë¡œ ìˆ˜í–‰)
                                    name = paper_name.strip() if paper_name.strip() else uploaded_file.name.replace('.pdf', '')
                                    st.session_state.papers[name] = {
                                        'text': text,
                                        'metadata': metadata,
                                        'summary': summary,
                                        'gpt_summary': None,  # ë‚˜ì¤‘ì— ìƒì„±
                                        'themes': None,
                                        'research_questions': None,
                                        'keywords': keywords,
                                        'references': references,
                                        'readability': readability,
                                        'complexity': complexity,
                                        'collocations': collocations,
                                        'discourse_markers': discourse,
                                        'citation_patterns': citations,
                                        'topics_lda': topics_lda
                                    }
                                    
                                    progress_bar.progress(100)
                                    status_text.text("âœ… ë¶„ì„ ì™„ë£Œ!")
                                    st.success(f"**'{name}'** ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                                    st.balloons()
        
        # ë¡œë“œëœ ë…¼ë¬¸ ëª©ë¡
        if st.session_state.papers:
            st.markdown("---")
            st.subheader("ğŸ“š ë¡œë“œëœ ë…¼ë¬¸")
            
            for idx, name in enumerate(st.session_state.papers.keys(), 1):
                col1, col2 = st.columns([4, 1])
                with col1:
                    pages = st.session_state.papers[name]['metadata']['pages']
                    st.write(f"**{idx}.** {name}")
                    if pages:
                        st.caption(f"ğŸ“„ {pages} í˜ì´ì§€")
                with col2:
                    if st.button("ğŸ—‘ï¸", key=f"del_{name}"):
                        del st.session_state.papers[name]
                        st.rerun()
            
            if len(st.session_state.papers) > 1:
                st.info(f"ğŸ’¡ {len(st.session_state.papers)}ê°œ ë…¼ë¬¸ ë¹„êµ ê°€ëŠ¥")
    
    # ë©”ì¸ ì˜ì—­: ê²°ê³¼ í‘œì‹œ
    if not st.session_state.papers:
        st.info("ğŸ‘ˆ **ì‹œì‘í•˜ê¸°:** ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.markdown("### ğŸ“ êµ¬ì¡°í™”ëœ ìš”ì•½")
            st.write("ë…¼ë¬¸ì˜ ì£¼ìš” ì„¹ì…˜ì„ ìë™ìœ¼ë¡œ ì‹ë³„í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤.")
        with col2:
            st.markdown("### ğŸ”‘ í‚¤ì›Œë“œ ë¶„ì„")
            st.write("TF-IDFì™€ ë¹ˆë„ ë¶„ì„ìœ¼ë¡œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.")
        with col3:
            st.markdown("### ğŸ“š ì°¸ê³ ë¬¸í—Œ ë¶„ì„")
            st.write("ì°¸ê³ ë¬¸í—Œì˜ ì—°ë„, ì €ì, ìœ í˜•ì„ ìƒì„¸íˆ ë¶„ì„í•©ë‹ˆë‹¤.")
    
    else:
        # ë…¼ë¬¸ ì„ íƒ
        selected_paper = st.selectbox(
            "ğŸ“– ë¶„ì„í•  ë…¼ë¬¸ ì„ íƒ",
            options=list(st.session_state.papers.keys()),
            key="paper_selector"
        )
        
        data = st.session_state.papers[selected_paper]
        
        # ë©”íƒ€ë°ì´í„° í‘œì‹œ
        meta = data['metadata']
        if meta['title'] or meta['author']:
            with st.expander("ğŸ“‹ ë¬¸ì„œ ì •ë³´", expanded=False):
                cols = st.columns(4)
                if meta['title']:
                    cols[0].metric("ì œëª©", meta['title'][:50] + "...")
                if meta['author']:
                    cols[1].metric("ì €ì", meta['author'][:30] + "...")
                if meta['pages']:
                    cols[2].metric("í˜ì´ì§€", meta['pages'])
                if meta['creator']:
                    cols[3].metric("ì‘ì„± ë„êµ¬", meta['creator'][:30])
        
        # íƒ­ ìƒì„±
        tabs = st.tabs([
            "ğŸ¤– AI ë¶„ì„",
            "ğŸ“Š ê°œìš”",
            "ğŸ“ˆ ê³ ê¸‰ ë¶„ì„",
            "ğŸ¯ ì£¼ì œ & ì—°êµ¬ì§ˆë¬¸",
            "ğŸ”‘ í‚¤ì›Œë“œ",
            "ğŸ“š ì°¸ê³ ë¬¸í—Œ",
            "ğŸ”„ ë¹„êµ ë¶„ì„"
        ])
        
        tab1, tab2, tab3, tab4, tab5, tab6, tab7 = tabs
        
        with tab1:
            st.markdown('<div class="section-header">ğŸ¤– GPT-4 ê¸°ë°˜ ì§€ëŠ¥í˜• ë¶„ì„</div>', unsafe_allow_html=True)
            
            # GPT ë¶„ì„ ë²„íŠ¼
            col1, col2 = st.columns([3, 1])
            with col1:
                st.info("ğŸ’¡ GPT ë¶„ì„ì€ ì„ íƒì ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë²„íŠ¼ì„ ëˆŒëŸ¬ AI ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")
            with col2:
                run_gpt = st.button("ğŸš€ GPT ë¶„ì„ ì‹¤í–‰", type="primary", key="gpt_analysis")
            
            if run_gpt:
                with st.spinner("ğŸ¤– GPTê°€ ë…¼ë¬¸ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (ì•½ 10-20ì´ˆ ì†Œìš”)"):
                    try:
                        gpt_summary = gpt_summarize(data['text'])
                        themes = gpt_extract_themes(data['text'])
                        research_qs = gpt_research_questions(data['text'])
                        
                        # ì„¸ì…˜ì— ì €ì¥
                        st.session_state.papers[selected_paper]['gpt_summary'] = gpt_summary
                        st.session_state.papers[selected_paper]['themes'] = themes
                        st.session_state.papers[selected_paper]['research_questions'] = research_qs
                        
                        st.success("âœ… GPT ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.rerun()
                    except Exception as e:
                        st.error(f"âŒ GPT ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                        st.warning("ğŸ’¡ API í• ë‹¹ëŸ‰ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ íƒ­ì—ì„œ ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            
            gpt_sum = data.get('gpt_summary', {})
            
            if gpt_sum is None:
                st.warning("âš ï¸ GPT ë¶„ì„ì´ ì•„ì§ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìœ„ì˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")
            elif 'error' in gpt_sum:
                st.error(gpt_sum['error'])
                st.info("ğŸ’¡ GPT ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ íƒ­ì—ì„œ ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            else:
                # í•µì‹¬ ìš”ì•½
                if 'í•µì‹¬ìš”ì•½' in gpt_sum:
                    st.markdown("#### ğŸ“ í•µì‹¬ ìš”ì•½")
                    st.info(gpt_sum['í•µì‹¬ìš”ì•½'])
                
                # êµ¬ì¡°í™”ëœ ì„¹ì…˜
                col1, col2 = st.columns(2)
                
                with col1:
                    if 'ì—°êµ¬ëª©ì ' in gpt_sum:
                        st.markdown("#### ğŸ¯ ì—°êµ¬ ëª©ì ")
                        st.write(gpt_sum['ì—°êµ¬ëª©ì '])
                    
                    if 'ì£¼ìš”ë°œê²¬' in gpt_sum:
                        st.markdown("#### ğŸ” ì£¼ìš” ë°œê²¬")
                        st.write(gpt_sum['ì£¼ìš”ë°œê²¬'])
                    
                    if 'í•œê³„ì ' in gpt_sum:
                        st.markdown("#### âš ï¸ ì—°êµ¬ í•œê³„")
                        st.write(gpt_sum['í•œê³„ì '])
                
                with col2:
                    if 'ì—°êµ¬ë°©ë²•' in gpt_sum:
                        st.markdown("#### ğŸ”¬ ì—°êµ¬ ë°©ë²•")
                        st.write(gpt_sum['ì—°êµ¬ë°©ë²•'])
                    
                    if 'ì´ë¡ ì ê¸°ì—¬' in gpt_sum:
                        st.markdown("#### ğŸ’¡ ì´ë¡ ì  ê¸°ì—¬")
                        st.write(gpt_sum['ì´ë¡ ì ê¸°ì—¬'])
        
        with tab2:
            st.markdown('<div class="section-header">ğŸ“Š ë…¼ë¬¸ ê°œìš”</div>', unsafe_allow_html=True)
            
            # ê¸°ë³¸ í†µê³„
            col1, col2, col3, col4 = st.columns(4)
            summary = data['summary']
            refs = data['references']
            
            col1.metric("ğŸ“„ í˜ì´ì§€ ìˆ˜", meta['pages'] if meta['pages'] else 'N/A')
            col2.metric("ğŸ“ ë‹¨ì–´ ìˆ˜", f"{summary['word_count']:,}")
            col3.metric("ğŸ’¬ ë¬¸ì¥ ìˆ˜", summary['sentence_count'])
            col4.metric("ğŸ“š ì°¸ê³ ë¬¸í—Œ", refs['count'])
            
            # í•µì‹¬ ìš”ì•½ (ê¸°ë³¸)
            st.markdown('<div class="section-header">í•µì‹¬ ìš”ì•½</div>', unsafe_allow_html=True)
            st.write(summary['executive'])
            
            # êµ¬ì¡°í™”ëœ ìš”ì•½
            st.markdown('<div class="section-header">êµ¬ì¡°í™”ëœ ìš”ì•½</div>', unsafe_allow_html=True)
            with st.expander("ì „ì²´ ë³´ê¸°", expanded=False):
                st.write(summary['structured'])
            
            # ìƒìœ„ í‚¤ì›Œë“œ ë¯¸ë¦¬ë³´ê¸°
            st.markdown('<div class="section-header">ì£¼ìš” í‚¤ì›Œë“œ (Top 10)</div>', unsafe_allow_html=True)
            if data['keywords']['tfidf']:
                keywords_preview = [kw[0] for kw in data['keywords']['tfidf'][:10]]
                st.write(" â€¢ ".join(keywords_preview))
            else:
                st.info("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        with tab3:
            st.markdown('<div class="section-header">ğŸ“ˆ ê³ ê¸‰ í…ìŠ¤íŠ¸ ë¶„ì„</div>', unsafe_allow_html=True)
            st.caption("Python NLP ê¸°ìˆ ì„ í™œìš©í•œ ì‹¬ì¸µ í…ìŠ¤íŠ¸ ë¶„ì„")
            
            # ê°€ë…ì„± ë¶„ì„
            readability = data.get('readability')
            if readability:
                st.markdown("#### ğŸ“– ê°€ë…ì„± ë¶„ì„")
                st.info(f"**ë‚œì´ë„:** {readability['difficulty']} (Flesch Reading Ease: {readability['flesch_reading_ease']})")
                
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("Flesch-Kincaid Grade", f"{readability['flesch_kincaid_grade']:.1f}")
                col2.metric("SMOG Index", f"{readability['smog_index']:.1f}")
                col3.metric("Coleman-Liau", f"{readability['coleman_liau']:.1f}")
                col4.metric("í‰ê·  í•™ë…„ ìˆ˜ì¤€", f"{readability['average_grade_level']:.1f}")
                
                with st.expander("â„¹ï¸ ê°€ë…ì„± ì§€í‘œ ì„¤ëª…"):
                    st.markdown("""
                    - **Flesch Reading Ease**: 0-100 ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì½ê¸° ì‰¬ì›€)
                      - 90-100: ë§¤ìš° ì‰¬ì›€ (ì´ˆë“± 5í•™ë…„)
                      - 60-70: í‘œì¤€ (ì¤‘í•™êµ 8-9í•™ë…„)
                      - 0-30: ë§¤ìš° ì–´ë ¤ì›€ (ëŒ€í•™ì› ìˆ˜ì¤€)
                    - **Grade Level ì§€í‘œë“¤**: ì´í•´ì— í•„ìš”í•œ êµìœ¡ ì—°ìˆ˜
                    - **í•™ìˆ  ë…¼ë¬¸**ì€ ì¼ë°˜ì ìœ¼ë¡œ ëŒ€í•™(13-16) ~ ëŒ€í•™ì›(17+) ìˆ˜ì¤€
                    """)
            
            # ë¬¸ì¥ ë³µì¡ë„ ë¶„ì„
            complexity = data.get('complexity')
            if complexity:
                st.markdown("#### ğŸ“Š ë¬¸ì¥ ë³µì¡ë„ ë¶„ì„")
                
                col1, col2, col3 = st.columns(3)
                col1.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´", f"{complexity['avg_sentence_length']:.1f} ë‹¨ì–´")
                col2.metric("í‰ê·  ë‹¨ì–´ ê¸¸ì´", f"{complexity['avg_word_length']:.1f} ê¸€ì")
                col3.metric("ì–´íœ˜ ë‹¤ì–‘ì„± (TTR)", f"{complexity['vocabulary_diversity']:.1f}%")
                
                col4, col5, col6 = st.columns(3)
                col4.metric("ì´ ë‹¨ì–´ ìˆ˜", f"{complexity['total_words']:,}")
                col5.metric("ê³ ìœ  ë‹¨ì–´ ìˆ˜", f"{complexity['unique_words']:,}")
                col6.metric("ê¸´ ë‹¨ì–´ ë¹„ìœ¨", f"{complexity['long_word_ratio']:.1f}%")
                
                # ë¬¸ì¥ ê¸¸ì´ ë¶„í¬ ì‹œê°í™”
                with st.expander("ğŸ“ˆ ë¬¸ì¥ ê¸¸ì´ í†µê³„"):
                    st.write(f"**ìµœì†Œ ê¸¸ì´:** {complexity['min_sentence_length']} ë‹¨ì–´")
                    st.write(f"**ìµœëŒ€ ê¸¸ì´:** {complexity['max_sentence_length']} ë‹¨ì–´")
                    st.write(f"**í‘œì¤€ í¸ì°¨:** {complexity['sentence_length_std']:.2f}")
            
            # Collocation ë¶„ì„
            collocations = data.get('collocations')
            if collocations:
                st.markdown("#### ğŸ”— ë‹¨ì–´ ì¡°í•© ë¶„ì„ (Collocations)")
                st.caption("í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ê²Œ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ìŒ")
                
                col1, col2 = st.columns(2)
                mid = len(collocations) // 2
                
                with col1:
                    for i, (collocation, freq) in enumerate(collocations[:mid], 1):
                        st.write(f"{i}. **{collocation}** `({freq}íšŒ)`")
                
                with col2:
                    for i, (collocation, freq) in enumerate(collocations[mid:], mid+1):
                        st.write(f"{i}. **{collocation}** `({freq}íšŒ)`")
            
            # ë‹´í™” í‘œì§€ ë¶„ì„
            discourse = data.get('discourse_markers')
            if discourse:
                st.markdown("#### ğŸ’¬ ë‹´í™” í‘œì§€ ë¶„ì„ (Discourse Markers)")
                st.caption("ë…¼ì¦ êµ¬ì¡°ì™€ ë…¼ë¦¬ ì „ê°œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì–¸ì–´ í‘œì§€")
                
                # ë°” ì°¨íŠ¸ë¡œ ì‹œê°í™”
                discourse_df = pd.DataFrame([
                    {'ì¹´í…Œê³ ë¦¬': k, 'ë¹ˆë„': v} for k, v in discourse.items()
                ])
                
                fig = px.bar(discourse_df, x='ë¹ˆë„', y='ì¹´í…Œê³ ë¦¬', 
                           orientation='h',
                           title='ë‹´í™” í‘œì§€ ì‚¬ìš© ë¹ˆë„',
                           color='ë¹ˆë„',
                           color_continuous_scale='blues')
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True)
                
                with st.expander("â„¹ï¸ ë‹´í™” í‘œì§€ ì„¤ëª…"):
                    st.markdown("""
                    - **ì¸ê³¼ê´€ê³„**: ì›ì¸ê³¼ ê²°ê³¼ë¥¼ ì—°ê²° (because, therefore, thus ë“±)
                    - **ëŒ€ì¡°**: ìƒë°˜ëœ ì•„ì´ë””ì–´ ì œì‹œ (however, but, although ë“±)
                    - **ì¶”ê°€**: ì •ë³´ ì¶”ê°€ (furthermore, moreover, also ë“±)
                    - **ì˜ˆì‹œ**: êµ¬ì²´ì  ì˜ˆì‹œ ì œê³µ (for example, such as ë“±)
                    - **ê²°ë¡ **: ë…¼ì§€ ë§ˆë¬´ë¦¬ (in conclusion, to sum up ë“±)
                    - **ê°•ì¡°**: ì£¼ì¥ ê°•í™” (indeed, in fact, clearly ë“±)
                    """)
            
            # LDA í† í”½ ëª¨ë¸ë§
            topics_lda = data.get('topics_lda')
            if topics_lda:
                st.markdown("#### ğŸ·ï¸ í† í”½ ëª¨ë¸ë§ (LDA)")
                st.caption("ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹ ê¸°ë²•ìœ¼ë¡œ ì¶”ì¶œí•œ ì£¼ìš” í† í”½")
                
                for topic in topics_lda:
                    with st.expander(f"**í† í”½ {topic['topic_id']}**", expanded=False):
                        st.write("**ì£¼ìš” ë‹¨ì–´:**")
                        words_with_scores = [f"{word} ({score:.3f})" 
                                           for word, score in zip(topic['words'][:5], topic['scores'][:5])]
                        st.write(" â€¢ ".join(words_with_scores))
                        
                        st.write("\n**ì „ì²´ ë‹¨ì–´:**")
                        st.write(", ".join(topic['words']))
            
            # ì¸ìš© íŒ¨í„´ ë¶„ì„
            citations = data.get('citation_patterns')
            if citations:
                st.markdown("#### ğŸ“ ì¸ìš© íŒ¨í„´ ë¶„ì„")
                
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("(Author, Year)", citations.get('author_year', 0))
                col2.metric("(Author, Year, p.X)", citations.get('author_year_page', 0))
                col3.metric("[ìˆ«ì]", citations.get('numbered', 0))
                col4.metric("et al. ì‚¬ìš©", citations.get('multiple_authors', 0))
                
                total_citations = sum(citations.values())
                if total_citations > 0:
                    st.info(f"ğŸ“Š **ì´ ì¸ìš© íšŸìˆ˜:** {total_citations}íšŒ")
                    
                    # ì¸ìš© ìŠ¤íƒ€ì¼ ë¹„ìœ¨
                    citation_df = pd.DataFrame([
                        {'ìŠ¤íƒ€ì¼': 'Author-Year', 'ë¹ˆë„': citations.get('author_year', 0)},
                        {'ìŠ¤íƒ€ì¼': 'Author-Year-Page', 'ë¹ˆë„': citations.get('author_year_page', 0)},
                        {'ìŠ¤íƒ€ì¼': 'Numbered', 'ë¹ˆë„': citations.get('numbered', 0)}
                    ])
                    
                    fig = px.pie(citation_df, values='ë¹ˆë„', names='ìŠ¤íƒ€ì¼',
                               title='ì¸ìš© ìŠ¤íƒ€ì¼ ë¶„í¬')
                    st.plotly_chart(fig, use_container_width=True)
        
        with tab4:
            st.markdown('<div class="section-header">ğŸ¯ ì£¼ì œ & ì—°êµ¬ì§ˆë¬¸</div>', unsafe_allow_html=True)
            
            # ì—°êµ¬ì§ˆë¬¸
            rqs = data.get('research_questions')
            
            if rqs is None:
                st.info("ğŸ’¡ 'AI ë¶„ì„' íƒ­ì—ì„œ GPT ë¶„ì„ì„ ì‹¤í–‰í•˜ë©´ ì—°êµ¬ì§ˆë¬¸ê³¼ ì£¼ì œë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.")
            elif 'error' not in rqs:
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("#### â“ ì—°êµ¬ì§ˆë¬¸")
                    if rqs.get('ì—°êµ¬ì§ˆë¬¸'):
                        for i, rq in enumerate(rqs['ì—°êµ¬ì§ˆë¬¸'], 1):
                            st.write(f"**RQ{i}:** {rq}")
                    else:
                        st.info("ì—°êµ¬ì§ˆë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                
                with col2:
                    st.markdown("#### ğŸ’­ ì—°êµ¬ê°€ì„¤")
                    if rqs.get('ì—°êµ¬ê°€ì„¤'):
                        for i, h in enumerate(rqs['ì—°êµ¬ê°€ì„¤'], 1):
                            st.write(f"**H{i}:** {h}")
                    else:
                        st.info("ì—°êµ¬ê°€ì„¤ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            # ì£¼ì œ ë¶„ì„
            themes = data.get('themes')
            
            if themes is not None and 'error' not in themes:
                st.markdown("#### ğŸ·ï¸ ì£¼ìš” ì£¼ì œ (Themes)")
                if themes.get('ì£¼ìš”ì£¼ì œ'):
                    cols = st.columns(3)
                    for i, theme in enumerate(themes['ì£¼ìš”ì£¼ì œ']):
                        cols[i % 3].info(f"**ì£¼ì œ {i+1}**\n\n{theme}")
                else:
                    st.info("ì£¼ì œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                
                st.markdown("#### ğŸ§© í•µì‹¬ ê°œë…")
                if themes.get('í•µì‹¬ê°œë…'):
                    concept_text = " â€¢ ".join(themes['í•µì‹¬ê°œë…'])
                    st.write(concept_text)
                else:
                    st.info("í•µì‹¬ ê°œë…ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            # ì„¹ì…˜ë³„ ë¶„ì„ (ê¸°ë³¸)
            st.markdown('<div class="section-header">ì„¹ì…˜ë³„ ë¶„ì„</div>', unsafe_allow_html=True)
            
            sections = summary['sections']
            selected_sections = ['ì—°êµ¬ ëª©ì  ë° ë°°ê²½', 'ì—°êµ¬ ë°©ë²•', 'ì—°êµ¬ ê²°ê³¼', 'ë…¼ì˜ ë° í•¨ì˜']
            
            for section_name in selected_sections:
                if section_name in sections:
                    section_data = sections[section_name]
                    with st.expander(f"**{section_name}**", expanded=False):
                        if section_data['content']:
                            for sent in section_data['content'][:3]:
                                st.write(f"â€¢ {sent}")
                        else:
                            st.info("ì´ ì„¹ì…˜ì˜ ë‚´ìš©ì„ ì‹ë³„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        with tab5:
            st.markdown('<div class="section-header">ğŸ”‘ í‚¤ì›Œë“œ ë¶„ì„</div>', unsafe_allow_html=True)
            
            keywords = data['keywords']
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### ğŸ“Š TF-IDF í‚¤ì›Œë“œ")
                st.caption("ë¬¸ì„œ ë‚´ ì¤‘ìš”ë„ ê¸°ë°˜ í‚¤ì›Œë“œ")
                
                if keywords['tfidf']:
                    for i, (keyword, score) in enumerate(keywords['tfidf'][:15], 1):
                        st.write(f"{i}. **{keyword}** `{score:.4f}`")
                else:
                    st.info("TF-IDF í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            with col2:
                st.markdown("#### ğŸ”¢ ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ")
                st.caption("ì¶œí˜„ ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ")
                
                if keywords['frequency']:
                    for i, (keyword, count) in enumerate(keywords['frequency'][:15], 1):
                        st.write(f"{i}. **{keyword}** `{count}íšŒ`")
                else:
                    st.info("ë¹ˆë„ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # í•™ìˆ  ìš©ì–´
            st.markdown("#### ğŸ“ í•™ìˆ  ë° ë°©ë²•ë¡  ìš©ì–´")
            st.caption("ì§ˆì /ì–‘ì  ì—°êµ¬ë°©ë²•ë¡  ê´€ë ¨ ìš©ì–´")
            
            if keywords['academic']:
                # 3ì—´ë¡œ í‘œì‹œ
                cols = st.columns(3)
                for i, (term, count) in enumerate(keywords['academic']):
                    col_idx = i % 3
                    cols[col_idx].write(f"**{term}** ({count})")
            else:
                st.info("í•™ìˆ  ìš©ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        with tab6:
            st.markdown('<div class="section-header">ğŸ“š ì°¸ê³ ë¬¸í—Œ ë¶„ì„</div>', unsafe_allow_html=True)
            
            refs = data['references']
            
            if refs['count'] == 0:
                st.warning("ì°¸ê³ ë¬¸í—Œ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                # í†µê³„ ìš”ì•½
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("ğŸ“š ì´ ì°¸ê³ ë¬¸í—Œ", refs['count'])
                col2.metric("ğŸ‘¥ í‰ê·  ì €ì ìˆ˜", refs['avg_authors'])
                col3.metric("ğŸ“… ìµœê·¼ 5ë…„ ë¹„ìœ¨", f"{refs['recent_ratio']}%")
                
                if refs['oldest_year'] and refs['newest_year']:
                    year_range = f"{refs['oldest_year']}-{refs['newest_year']}"
                    col4.metric("ğŸ“† ì—°ë„ ë²”ìœ„", year_range)
                
                # ì—°ë„ë³„ ë¶„í¬
                st.markdown("#### ğŸ“… ì—°ë„ë³„ ì°¸ê³ ë¬¸í—Œ ë¶„í¬")
                if refs['years']:
                    st.bar_chart(refs['years'])
                else:
                    st.info("ì—°ë„ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                # ì¶œíŒë¬¼ ìœ í˜•
                if refs['journal_types']:
                    st.markdown("#### ğŸ“– ì¶œíŒë¬¼ ìœ í˜• ë¶„í¬")
                    col1, col2 = st.columns([2, 1])
                    with col1:
                        st.bar_chart(refs['journal_types'])
                    with col2:
                        for j_type, count in refs['journal_types'].items():
                            percentage = (count / refs['count'] * 100)
                            st.write(f"**{j_type}**: {count}ê°œ ({percentage:.1f}%)")
                
                # ì°¸ê³ ë¬¸í—Œ ëª©ë¡
                st.markdown("#### ğŸ“‹ ì°¸ê³ ë¬¸í—Œ ëª©ë¡ (ìƒìœ„ 20ê°œ)")
                with st.expander("ì „ì²´ ëª©ë¡ ë³´ê¸°", expanded=False):
                    for i, ref in enumerate(refs['items'], 1):
                        st.write(f"{i}. {ref}")
        
        with tab7:
            st.markdown('<div class="section-header">ğŸ”„ ë…¼ë¬¸ ë¹„êµ ë¶„ì„</div>', unsafe_allow_html=True)
            
            if len(st.session_state.papers) < 2:
                st.info("ğŸ’¡ ë¹„êµ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ë…¼ë¬¸ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
                st.markdown("""
                **ğŸ¤– AI ê¸°ë°˜ ë¹„êµ ë¶„ì„ ê¸°ëŠ¥:**
                - GPT-4ë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• ë…¼ë¬¸ ë¹„êµ
                - ê³µí†µ ì£¼ì œ ë° ì°¨ë³„ì  ì‹ë³„
                - ì—°êµ¬ë°©ë²•ë¡  ë¹„êµ
                - ê¸°ë³¸ í†µê³„ ë¹„êµ
                - í‚¤ì›Œë“œ ë° ì°¸ê³ ë¬¸í—Œ íŒ¨í„´ ë¶„ì„
                """)
            else:
                # GPT ê¸°ë°˜ ë¹„êµ
                st.markdown("#### ğŸ¤– AI ê¸°ë°˜ ì‹¬ì¸µ ë¹„êµ")
                
                if st.button("ğŸš€ GPT ë¹„êµ ë¶„ì„ ì‹¤í–‰", type="primary", key="gpt_compare"):
                    with st.spinner("ğŸ¤– GPTê°€ ë…¼ë¬¸ë“¤ì„ ë¹„êµ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (ì•½ 20-30ì´ˆ ì†Œìš”)"):
                        try:
                            paper_texts = {name: data['text'] for name, data in st.session_state.papers.items()}
                            gpt_comp = gpt_compare_papers(paper_texts)
                            
                            if 'error' not in gpt_comp:
                                st.success("âœ… AI ë¹„êµ ë¶„ì„ ì™„ë£Œ!")
                                
                                if 'ê³µí†µì£¼ì œ' in gpt_comp:
                                    st.markdown("##### ğŸ¯ ê³µí†µ ì£¼ì œ")
                                    for theme in gpt_comp['ê³µí†µì£¼ì œ']:
                                        st.write(f"â€¢ {theme}")
                                
                                if 'ì°¨ë³„ì ' in gpt_comp:
                                    st.markdown("##### ğŸ” ì£¼ìš” ì°¨ë³„ì ")
                                    st.info(gpt_comp['ì°¨ë³„ì '])
                                
                                if 'ë°©ë²•ë¡ ë¹„êµ' in gpt_comp:
                                    st.markdown("##### ğŸ”¬ ë°©ë²•ë¡  ë¹„êµ")
                                    st.write(gpt_comp['ë°©ë²•ë¡ ë¹„êµ'])
                                
                                if 'ì¢…í•©í‰ê°€' in gpt_comp:
                                    st.markdown("##### ğŸ“Š ì¢…í•© í‰ê°€")
                                    st.success(gpt_comp['ì¢…í•©í‰ê°€'])
                            else:
                                st.error(gpt_comp['error'])
                                st.warning("ğŸ’¡ API í• ë‹¹ëŸ‰ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ê¸°ë³¸ ë¹„êµ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                        except Exception as e:
                            st.error(f"âŒ GPT ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                            st.warning("ğŸ’¡ ì•„ë˜ ê¸°ë³¸ ë¹„êµ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                
                st.markdown("---")
                
                # ê¸°ë³¸ í†µê³„ ë¹„êµ
                comparison = compare_papers(st.session_state.papers)
                
                if comparison:
                    st.markdown("#### ğŸ“Š ê¸°ë³¸ í†µê³„ ë¹„êµ")
                    st.table(comparison['basic_stats'])
                    
                    # í‚¤ì›Œë“œ ë¹„êµ
                    st.markdown("#### ğŸ”‘ ê³µí†µ í‚¤ì›Œë“œ")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**TF-IDF ê³µí†µ í‚¤ì›Œë“œ:**")
                        if comparison['common_keywords']['tfidf']:
                            st.write(" â€¢ ".join(comparison['common_keywords']['tfidf']))
                        else:
                            st.info("ê³µí†µ í‚¤ì›Œë“œ ì—†ìŒ")
                    
                    with col2:
                        st.markdown("**í•™ìˆ  ìš©ì–´ ê³µí†µ í‚¤ì›Œë“œ:**")
                        if comparison['common_keywords']['academic']:
                            st.write(" â€¢ ".join(comparison['common_keywords']['academic']))
                        else:
                            st.info("ê³µí†µ í•™ìˆ  ìš©ì–´ ì—†ìŒ")
                    
                    # ì°¸ê³ ë¬¸í—Œ ë¹„êµ
                    st.markdown("#### ğŸ“š ì°¸ê³ ë¬¸í—Œ ë¹„êµ")
                    st.table(comparison['references'])
                    
                    # ì—°êµ¬ë°©ë²•ë¡  ë¹„êµ
                    st.markdown("#### ğŸ”¬ ì—°êµ¬ë°©ë²•ë¡  ìš©ì–´ ë¹„êµ")
                    for paper_name, terms in comparison['methodology'].items():
                        with st.expander(f"**{paper_name}**"):
                            if terms:
                                st.write(" â€¢ ".join(terms))
                            else:
                                st.info("ë°©ë²•ë¡  ê´€ë ¨ ìš©ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
